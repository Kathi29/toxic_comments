{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ekaterinakastaleva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "toxic_comments = pd.read_csv(\"/Users/ekaterinakastaleva/russian_comments_from_2ch_pikabu.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>translated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Верблюдов-то за что? Дебилы, бл...\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Camels, for what? Morons, bl ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ukrainians, this is an outlet vent zatyukanogo Russians, they say, out, and Ukrainians worse. If there were no Ukrainians, jelly would have invented them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Собаке - собачья смерть\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dog - Dog Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Refresh the page, moron. This is also not an insult, but a proven fact - I will not write a non-moron to myself in the plural. Or do we believe in you - are you and your imaginary friends?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>тебя не убедил 6-страничный пдф в том, что Скрипалей отравила Россия? Анализировать и думать пытаешься? Ватник что ли?)\\n</td>\n",
       "      <td>1.0</td>\n",
       "      <td>did not convince you of the 6-page pdf that Skripale was poisoned by Russia? Are you trying to analyze and think? Padded jacket or what?)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                     comment  \\\n",
       "0                                                                                                                                                       Верблюдов-то за что? Дебилы, бл...\\n   \n",
       "1                                                                 Хохлы, это отдушина затюканого россиянина, мол, вон, а у хохлов еще хуже. Если бы хохлов не было, кисель их бы придумал.\\n   \n",
       "2                                                                                                                                                                  Собаке - собачья смерть\\n   \n",
       "3  Страницу обнови, дебил. Это тоже не оскорбление, а доказанный факт - не-дебил про себя во множественном числе писать не будет. Или мы в тебя верим - это ты и твои воображаемые друзья?\\n   \n",
       "4                                                                  тебя не убедил 6-страничный пдф в том, что Скрипалей отравила Россия? Анализировать и думать пытаешься? Ватник что ли?)\\n   \n",
       "\n",
       "   toxic  \\\n",
       "0    1.0   \n",
       "1    1.0   \n",
       "2    1.0   \n",
       "3    1.0   \n",
       "4    1.0   \n",
       "\n",
       "                                                                                                                                                                                       translated  \n",
       "0                                                                                                                                                               Camels, for what? Morons, bl ...   \n",
       "1                                     Ukrainians, this is an outlet vent zatyukanogo Russians, they say, out, and Ukrainians worse. If there were no Ukrainians, jelly would have invented them.   \n",
       "2                                                                                                                                                                                Dog - Dog Death   \n",
       "3   Refresh the page, moron. This is also not an insult, but a proven fact - I will not write a non-moron to myself in the plural. Or do we believe in you - are you and your imaginary friends?   \n",
       "4                                                      did not convince you of the 6-page pdf that Skripale was poisoned by Russia? Are you trying to analyze and think? Padded jacket or what?)   "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment', 'toxic'], dtype='object')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.columns=[ \"comment\", \"toxic\",\"translated\"]\n",
    "#I dont need the translated-column, because of the bad translation\n",
    "toxic_comments=toxic_comments[[\"comment\",\"toxic\"]]\n",
    "toxic_comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "other_exclusions = [\"#ff\", \"ff\", \"rt\"]\n",
    "russian_stopwords.extend(other_exclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(string):\n",
    "    space_pattern = '\\s+'\n",
    "    url = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    parsed_text = re.sub(space_pattern, ' ', string)\n",
    "    parsed_text = re.sub(url, '', parsed_text)\n",
    "    parsed_text = re.sub(mention_regex, '', parsed_text)\n",
    "    return parsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Это моя почта kashtaliowa.com и мой сайт '"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(\"Это моя почта kashtaliowa@gmail.com и мой сайт http://wwww.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "russian_stopwords = stopwords.words(\"russian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, threshold = 1):\n",
    "    threshold = threshold\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token !=\" \"\\\n",
    "             and token.strip() not in punctuation]\n",
    "    text = \" \".join([word for word in tokens if len(word) > threshold])\n",
    "    return re.sub('[0-9]+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'сегодня быть магазин'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('Я сегодня был в магазине.') #function tokenize works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Для каких стан является эталоном современная система здравоохранения РФ? Для Зимбабве? Ты тупой? хохлы\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>УПАД Т! ТАМ НЕЛЬЗЯ СТРОИТЬ! ТЕХНОЛОГИЙ НЕТ! РАЗВОРУЮТ КАК ВСЕГДА! УЖЕ ТРЕЩИНАМИ ПОШ Л! ТУПЫЕ КИТАЗЫ НЕ МОГУТ НИЧЕГО НОРМАЛЬНО СДЕЛАТЬ!\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ебать тебя разносит, шизик.\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Обосрался, сиди обтекай\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Зачем ты пишешь хуйню, дегенерат? Поцелуй в губы ! поцелую в засос.\\n</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     comment  \\\n",
       "5                                   Для каких стан является эталоном современная система здравоохранения РФ? Для Зимбабве? Ты тупой? хохлы\\n   \n",
       "7   УПАД Т! ТАМ НЕЛЬЗЯ СТРОИТЬ! ТЕХНОЛОГИЙ НЕТ! РАЗВОРУЮТ КАК ВСЕГДА! УЖЕ ТРЕЩИНАМИ ПОШ Л! ТУПЫЕ КИТАЗЫ НЕ МОГУТ НИЧЕГО НОРМАЛЬНО СДЕЛАТЬ!\\n   \n",
       "8                                                                                                              Ебать тебя разносит, шизик.\\n   \n",
       "9                                                                                                                  Обосрался, сиди обтекай\\n   \n",
       "10                                                                     Зачем ты пишешь хуйню, дегенерат? Поцелуй в губы ! поцелую в засос.\\n   \n",
       "\n",
       "    toxic  \n",
       "5     1.0  \n",
       "7     1.0  \n",
       "8     1.0  \n",
       "9     1.0  \n",
       "10    1.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_toxic_comments = toxic_comments[toxic_comments[\"toxic\"]==1.0]\n",
    "print(len(only_toxic_comments))\n",
    "only_toxic_comments[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    9586\n",
       "1.0    4826\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_comments = toxic_comments[toxic_comments[\"toxic\"]==0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(toxic_comments.isnull().any(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4800 comments with non toxic meaning.\n",
      "There are 4800 comments with toxic meaning.\n"
     ]
    }
   ],
   "source": [
    "#I have now two classes: normal_comments and only_toxic_comments\n",
    "normal_comments = normal_comments.iloc[:int(4800)]\n",
    "print(\"There are \" + str(len(normal_comments)) + \" comments with non toxic meaning.\")\n",
    "only_toxic_comments = only_toxic_comments.iloc[:int(4800)]\n",
    "print(\"There are \" + str(len(only_toxic_comments)) + \" comments with toxic meaning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9600\n"
     ]
    }
   ],
   "source": [
    "toxic_dataset = pd.concat([only_toxic_comments, normal_comments])\n",
    "print(len(toxic_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF verctorarization\n",
    "#tokenizer?\n",
    "vectorizer = TfidfVectorizer(\n",
    "tokenizer = tokenize,\n",
    "preprocessor = preprocessing,\n",
    "ngram_range = (1, 3),\n",
    "stop_words = russian_stopwords,\n",
    "use_idf = True,\n",
    "smooth_idf = False,\n",
    "norm = None,\n",
    "decode_error = 'replace',\n",
    "max_features = 10000,\n",
    "max_df = 1, #max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "min_df = 1) #min_df = 5 means \"ignore terms that appear in less than 5 documents\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['б', 'г', 'д', 'е', 'з', 'й', 'л', 'м', 'н', 'п', 'р', 'т', 'х', 'ц', 'ч', 'ш', 'щ', 'ы', 'ь', 'э'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf = vectorizer.fit_transform(toxic_dataset).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 2)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 0.\n",
      "  0.         3.38629436 1.69314718 1.69314718 1.69314718 1.69314718\n",
      "  1.69314718 1.69314718 1.69314718 1.69314718 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         1.69314718\n",
      "  1.69314718 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         1.69314718 1.69314718\n",
      "  1.69314718 1.69314718 1.69314718 1.69314718 1.69314718]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
